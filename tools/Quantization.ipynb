{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/vovacher/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:root:Keras version 2.3.1 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import coremltools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CoreML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = coremltools.models.MLModel(\n",
    "    f\"../vision/classification/resnet_v1/models/resnet18_v1_torchvision.mlmodel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize CoreML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following quantizations are [supported](https://apple.github.io/coremltools/generated/coremltools.models.neural_network.quantization_utils.html#coremltools.models.neural_network.quantization_utils.quantize_weights):\n",
    "* **FP16**\n",
    "* **1-8 bit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer input_1_scaler_custom\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer BatchNormalization_1\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer BatchNormalization_5\n",
      "Quantizing layer Conv_7\n",
      "Quantizing layer BatchNormalization_8\n",
      "Quantizing layer Conv_11\n",
      "Quantizing layer BatchNormalization_12\n",
      "Quantizing layer Conv_14\n",
      "Quantizing layer BatchNormalization_15\n",
      "Quantizing layer Conv_18\n",
      "Quantizing layer BatchNormalization_19\n",
      "Quantizing layer Conv_21\n",
      "Quantizing layer BatchNormalization_22\n",
      "Quantizing layer Conv_23\n",
      "Quantizing layer BatchNormalization_24\n",
      "Quantizing layer Conv_27\n",
      "Quantizing layer BatchNormalization_28\n",
      "Quantizing layer Conv_30\n",
      "Quantizing layer BatchNormalization_31\n",
      "Quantizing layer Conv_34\n",
      "Quantizing layer BatchNormalization_35\n",
      "Quantizing layer Conv_37\n",
      "Quantizing layer BatchNormalization_38\n",
      "Quantizing layer Conv_39\n",
      "Quantizing layer BatchNormalization_40\n",
      "Quantizing layer Conv_43\n",
      "Quantizing layer BatchNormalization_44\n",
      "Quantizing layer Conv_46\n",
      "Quantizing layer BatchNormalization_47\n",
      "Quantizing layer Conv_50\n",
      "Quantizing layer BatchNormalization_51\n",
      "Quantizing layer Conv_53\n",
      "Quantizing layer BatchNormalization_54\n",
      "Quantizing layer Conv_55\n",
      "Quantizing layer BatchNormalization_56\n",
      "Quantizing layer Conv_59\n",
      "Quantizing layer BatchNormalization_60\n",
      "Quantizing layer Conv_62\n",
      "Quantizing layer BatchNormalization_63\n",
      "Quantizing layer Gemm_68\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = coremltools.utils.convert_neural_network_weights_to_fp16(model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing using linear quantization\n",
      "Optimizing Neural Network before Quantization:\n",
      "Fused Conv_0->BatchNormalization_1\n",
      "Fused Conv_4->BatchNormalization_5\n",
      "Fused Conv_7->BatchNormalization_8\n",
      "Fused Conv_11->BatchNormalization_12\n",
      "Fused Conv_14->BatchNormalization_15\n",
      "Fused Conv_18->BatchNormalization_19\n",
      "Fused Conv_21->BatchNormalization_22\n",
      "Fused Conv_23->BatchNormalization_24\n",
      "Fused Conv_27->BatchNormalization_28\n",
      "Fused Conv_30->BatchNormalization_31\n",
      "Fused Conv_34->BatchNormalization_35\n",
      "Fused Conv_37->BatchNormalization_38\n",
      "Fused Conv_39->BatchNormalization_40\n",
      "Fused Conv_43->BatchNormalization_44\n",
      "Fused Conv_46->BatchNormalization_47\n",
      "Fused Conv_50->BatchNormalization_51\n",
      "Fused Conv_53->BatchNormalization_54\n",
      "Fused Conv_55->BatchNormalization_56\n",
      "Fused Conv_59->BatchNormalization_60\n",
      "Fused Conv_62->BatchNormalization_63\n",
      "Finished optimizing network. Quantizing neural network..\n",
      "Quantizing layer input_1_scaler_custom\n",
      "Quantizing layer Conv_0\n",
      "Quantizing layer Conv_4\n",
      "Quantizing layer Conv_7\n",
      "Quantizing layer Conv_11\n",
      "Quantizing layer Conv_14\n",
      "Quantizing layer Conv_18\n",
      "Quantizing layer Conv_21\n",
      "Quantizing layer Conv_23\n",
      "Quantizing layer Conv_27\n",
      "Quantizing layer Conv_30\n",
      "Quantizing layer Conv_34\n",
      "Quantizing layer Conv_37\n",
      "Quantizing layer Conv_39\n",
      "Quantizing layer Conv_43\n",
      "Quantizing layer Conv_46\n",
      "Quantizing layer Conv_50\n",
      "Quantizing layer Conv_53\n",
      "Quantizing layer Conv_55\n",
      "Quantizing layer Conv_59\n",
      "Quantizing layer Conv_62\n",
      "Quantizing layer Gemm_68\n"
     ]
    }
   ],
   "source": [
    "model_8bit = coremltools.models.neural_network.quantization_utils.quantize_weights(model_fp32, \n",
    "                                                                                   nbits=8, \n",
    "                                                                                   quantization_mode=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filelist = sorted(glob.glob(\"../data/images/test/*.jpg\"))\n",
    "len(test_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_fp32_all = []\n",
    "preds_fp16_all = []\n",
    "preds_8bit_all = []\n",
    "for file in test_filelist:\n",
    "    _image = Image.open(file)\n",
    "    _image = _image.resize((224, 224))\n",
    "\n",
    "    _preds_fp32 = model_fp32.predict({\"input_1\": _image}, useCPUOnly=True)[\"output_1\"]\n",
    "    _preds_fp32_array = []\n",
    "    for cl in sorted(_preds_fp32.keys()):\n",
    "        _preds_fp32_array.append(_preds_fp32[cl])\n",
    "    preds_fp32_all.append(np.array([_preds_fp32_array], dtype=\"float32\"))\n",
    "\n",
    "    _preds_fp16 = model_fp16.predict({\"input_1\": _image}, useCPUOnly=True)[\"output_1\"]\n",
    "    _preds_fp16_array = []\n",
    "    for cl in sorted(_preds_fp32.keys()):\n",
    "        _preds_fp16_array.append(_preds_fp16[cl])\n",
    "    preds_fp16_all.append(np.array([_preds_fp16_array], dtype=\"float32\"))\n",
    "\n",
    "    _preds_8bit = model_8bit.predict({\"input_1\": _image}, useCPUOnly=True)[\"output_1\"]\n",
    "    _preds_8bit_array = []\n",
    "    for cl in sorted(_preds_fp32.keys()):\n",
    "        _preds_8bit_array.append(_preds_8bit[cl])\n",
    "    preds_8bit_all.append(np.array([_preds_8bit_array], dtype=\"float32\"))\n",
    "\n",
    "preds_fp32_all = np.concatenate(preds_fp32_all)\n",
    "preds_fp16_all = np.concatenate(preds_fp16_all)\n",
    "preds_8bit_all = np.concatenate(preds_8bit_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP32 (full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(abs(preds_fp32_all - preds_fp32_all) / preds_fp32_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006121083"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(abs(preds_fp32_all - preds_fp16_all) / preds_fp32_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12689283"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(abs(preds_fp32_all - preds_8bit_all) / preds_fp32_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
